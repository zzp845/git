{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8bc2cc5",
   "metadata": {},
   "source": [
    "## Классификация текстов\n",
    "Цель работы: изучить методы классификации текстов на основе различных способов векторизации.\n",
    "\n",
    "Задание: Реализовать два подхода к классификации текстов:\n",
    "\n",
    "- На основе TfidfVectorizer.\n",
    "- На основе word2vec, обученного вручную на корпусе.\n",
    "- Сравнить полученные результаты классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb848a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.24.3\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd7e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import nltk\n",
    "# import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d75a55",
   "metadata": {},
   "source": [
    "IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n",
    "\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d36203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'IMDB_Dataset.csv'\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2067d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba0af6",
   "metadata": {},
   "source": [
    "## Способ 1. Классификация на основе TfidfVectorizer\n",
    "Для первого подхода тексты были преобразованы в числовые векторы с помощью TfidfVectorizer, исключающего стоп-слова и учитывающего частотность слов.\n",
    "\n",
    "Классификация выполнялась с помощью модели LogisticRegression, которая хорошо работает на задачах с разреженными признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb33665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.87      0.89      4961\n",
      "    positive       0.88      0.91      0.89      5039\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Разделяем на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Векторизация текста\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Обучение логистической регрессии\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Оценка модели\n",
    "y_pred = model.predict(X_test_vec)\n",
    "target_names = [str(cls) for cls in np.unique(y_test)]\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aac8973",
   "metadata": {},
   "source": [
    "## Способ 2. Классификация на основе Word2Vec\n",
    "\n",
    "Во втором подходе используется модель Word2Vec, обученная на тексте из датасета 20 Newsgroups.\n",
    "Каждое слово кодируется вектором, отражающим его контекст, а вектор документа получаем как среднее всех слововых векторов.\n",
    "\n",
    "Обучение выполняется с использованием библиотеки gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df183583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/artisia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/artisia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/artisia/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')  # иногда помогает отдельно\n",
    "# nltk.download('popular')    # качает всё базовое, включая punkt и токенизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24ff91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизируем все тексты для обучения модели\n",
    "tokenized_texts = [word_tokenize(text.lower()) for text in df['review']]\n",
    "\n",
    "# Обучаем Word2Vec на корпусе\n",
    "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=2, workers=4, sg=1)\n",
    "\n",
    "# Функция усреднения эмбеддингов слов в документе\n",
    "def document_vector(doc, model):\n",
    "    words = [word for word in word_tokenize(doc.lower()) if word in model.wv.key_to_index]\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(model.wv[words], axis=0)\n",
    "\n",
    "# Преобразуем все документы в векторы\n",
    "X_w2v = np.array([document_vector(text, w2v_model) for text in df['review']])\n",
    "y = df['sentiment']\n",
    "\n",
    "# Делим выборку\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3080cf",
   "metadata": {},
   "source": [
    "После получения векторного представления документов, была обучена модель LogisticRegression.\n",
    "Модель показала определённый уровень качества классификации, позволяющий сравнить её с результатами, полученными при использовании TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06092d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.86      0.86      4961\n",
      "    positive       0.86      0.87      0.87      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.87      0.86      0.86     10000\n",
      "weighted avg       0.87      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Обучаем модель на векторах Word2Vec\n",
    "model_w2v = LogisticRegression(max_iter=1000)\n",
    "model_w2v.fit(X_train_w2v, y_train_w2v)\n",
    "\n",
    "# Предсказания\n",
    "y_pred_w2v = model_w2v.predict(X_test_w2v)\n",
    "\n",
    "# Выводим отчёт\n",
    "print(classification_report(y_test_w2v, y_pred_w2v, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09882c93",
   "metadata": {},
   "source": [
    "## Сравнение моделей и вывод\n",
    "\n",
    "В данной лабораторной работе были реализованы два подхода к классификации текстов:\n",
    "\n",
    "TfidfVectorizer + LogisticRegression\n",
    "Модель показала высокую точность благодаря учёту частоты слов в тексте и их значимости в корпусе.\n",
    "Преимущество — простота и эффективность на небольших и средних наборах данных.\n",
    "\n",
    "Word2Vec + LogisticRegression\n",
    "Этот подход использует контекстные представления слов, усреднённые по каждому документу.\n",
    "Хотя результат может быть ниже, модель способна улавливать семантику слов и подходит для более глубокого анализа текста.\n",
    "\n",
    "Обе модели успешно решили задачу классификации. Метод на основе TfidfVectorizer показал более высокую точность на выбранном корпусе, однако Word2Vec даёт более гибкое представление текста и может быть полезен при работе с более сложными задачами или большими данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b6d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
