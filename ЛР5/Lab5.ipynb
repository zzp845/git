{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b26a65",
   "metadata": {},
   "source": [
    "# Лабораторная работа №5: Предобработка текста\n",
    "**Выполнил:** Чжан Цзэнпэн, ИУ5И-21М"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef01b42",
   "metadata": {},
   "source": [
    "## Цель лабораторной работы:\n",
    "Изучение методов предобработки текстов.\n",
    "\n",
    "## Задание:\n",
    "Для произвольного предложения или текста решите следующие задачи:\n",
    "1. Токенизация.\n",
    "2. Частеречная разметка.\n",
    "3. Лемматизация.\n",
    "4. Выделение (распознавание) именованных сущностей.\n",
    "5. Разбор предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fe23431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download ru_core_news_sm # Для русского языка (маленькая модель)\n",
    "# # или\n",
    "# # python -m spacy download en_core_web_sm # Для английского языка (маленькая модель)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f6a4c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Русскоязычная модель 'ru_core_news_sm' успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Загрузка русскоязычной модели (или en_core_web_sm для английского)\n",
    "# Убедись, что модель скачана (см. инструкции по установке выше)\n",
    "try:\n",
    "    nlp = spacy.load(\"ru_core_news_sm\")\n",
    "    print(\"Русскоязычная модель 'ru_core_news_sm' успешно загружена.\")\n",
    "except OSError:\n",
    "    print(\"Модель 'ru_core_news_sm' не найдена. Установите ее командой:\")\n",
    "    print(\"python -m spacy download ru_core_news_sm\")\n",
    "    print(\"И перезапустите ядро Jupyter.\")\n",
    "    # Для продолжения работы без установленной модели (ограниченная функциональность)\n",
    "    # nlp = spacy.blank(\"ru\") # Создает пустой конвейер для русского языка\n",
    "    # print(\"Создан пустой русскоязычный конвейер. Функциональность будет ограничена.\")\n",
    "    # Лучше установить модель!\n",
    "    nlp = None # Чтобы последующий код не падал, если модель не загружена\n",
    "\n",
    "# Если работаете с английским текстом:\n",
    "# try:\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     print(\"Англоязычная модель 'en_core_web_sm' успешно загружена.\")\n",
    "# except OSError:\n",
    "#     print(\"Модель 'en_core_web_sm' не найдена. Установите ее командой:\")\n",
    "#     print(\"python -m spacy download en_core_web_sm\")\n",
    "#     nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b809dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Будем обрабатывать текст: 'Мама мыла раму вчера вечером в Москве, а Билл Гейтс основал компанию Microsoft.'\n"
     ]
    }
   ],
   "source": [
    "# Определение текста для анализа\n",
    "# Пример предложения на русском языке\n",
    "text_ru = \"Мама мыла раму вчера вечером в Москве, а Билл Гейтс основал компанию Microsoft.\"\n",
    "\n",
    "# Пример предложения на английском языке (если используется en_core_web_sm)\n",
    "# text_en = \"Apple is looking at buying U.K. startup for $1 billion in London.\"\n",
    "\n",
    "# Выбираем текст для работы\n",
    "# Если русская модель загружена, используем русский текст\n",
    "if nlp and nlp.lang == 'ru':\n",
    "    text_to_process = text_ru\n",
    "    print(f\"\\nБудем обрабатывать текст: '{text_to_process}'\")\n",
    "elif nlp and nlp.lang == 'en':\n",
    "    # text_to_process = text_en\n",
    "    # print(f\"\\nБудем обрабатывать текст: '{text_to_process}'\")\n",
    "    # Для примера оставим русский текст, но поменяем модель на английскую если русская не загрузилась\n",
    "    # Лучше использовать текст, соответствующий языку модели!\n",
    "    print(\"Русская модель не загружена, но есть английская. Пример текста не изменен.\")\n",
    "    print(\"Для корректной работы с английской моделью измените text_to_process на text_en.\")\n",
    "    text_to_process = text_ru # Оставим для демонстрации, но результаты будут некорректны\n",
    "else:\n",
    "    print(\"\\nЯзыковая модель не загружена. Пожалуйста, установите модель и перезапустите ядро.\")\n",
    "    text_to_process = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a328bb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Текст успешно обработан.\n"
     ]
    }
   ],
   "source": [
    "# Обработка текста с помощью spaCy\n",
    "# Обрабатываем текст\n",
    "# doc - это объект, содержащий обработанную информацию о тексте\n",
    "if nlp and text_to_process:\n",
    "    doc = nlp(text_to_process)\n",
    "    print(\"\\nТекст успешно обработан.\")\n",
    "else:\n",
    "    doc = None\n",
    "    print(\"\\nТекст не может быть обработан, так как модель не загружена или текст пуст.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e55f5ee",
   "metadata": {},
   "source": [
    "## 1. Токенизация\n",
    "Разбиение текста на отдельные единицы — токены (обычно слова и знаки препинания)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b284550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Токенизация ---\n",
      "Токены: ['Мама', 'мыла', 'раму', 'вчера', 'вечером', 'в', 'Москве', ',', 'а', 'Билл', 'Гейтс', 'основал', 'компанию', 'Microsoft', '.']\n",
      "Всего токенов: 15\n"
     ]
    }
   ],
   "source": [
    "# Вывод токенов\n",
    "if doc:\n",
    "    print(\"\\n--- 1. Токенизация ---\")\n",
    "    tokens = [token.text for token in doc]\n",
    "    print(\"Токены:\", tokens)\n",
    "    print(f\"Всего токенов: {len(tokens)}\")\n",
    "else:\n",
    "    print(\"\\nТокенизация невозможна без обработанного документа.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a640e883",
   "metadata": {},
   "source": [
    "Tекст был разбит на отдельные слова и знаки препинания. Каждое слово и каждый знак препинания стали отдельным токеном."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd1b14",
   "metadata": {},
   "source": [
    "## 2. Частеречная разметка (Part-of-Speech Tagging)\n",
    "Определение части речи для каждого токена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39f1a7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Частеречная разметка (POS Tagging) ---\n",
      "Токен           | Часть речи (Coarse)  | Часть речи (Fine)    | Описание (Fine)\n",
      "----------------------------------------------------------------------\n",
      "Мама            | NOUN                 | NOUN                 | noun\n",
      "мыла            | VERB                 | VERB                 | verb\n",
      "раму            | NOUN                 | NOUN                 | noun\n",
      "вчера           | ADV                  | ADV                  | adverb\n",
      "вечером         | NOUN                 | NOUN                 | noun\n",
      "в               | ADP                  | ADP                  | adposition\n",
      "Москве          | PROPN                | PROPN                | proper noun\n",
      ",               | PUNCT                | PUNCT                | punctuation\n",
      "а               | CCONJ                | CCONJ                | coordinating conjunction\n",
      "Билл            | PROPN                | PROPN                | proper noun\n",
      "Гейтс           | PROPN                | PROPN                | proper noun\n",
      "основал         | VERB                 | VERB                 | verb\n",
      "компанию        | NOUN                 | NOUN                 | noun\n",
      "Microsoft       | PROPN                | PROPN                | proper noun\n",
      ".               | PUNCT                | PUNCT                | punctuation\n"
     ]
    }
   ],
   "source": [
    "# Вывод токенов с их частями речи\n",
    "if doc:\n",
    "    print(\"\\n--- 2. Частеречная разметка (POS Tagging) ---\")\n",
    "    print(f\"{'Токен':<15} | {'Часть речи (Coarse)':<20} | {'Часть речи (Fine)':<20} | {'Описание (Fine)'}\")\n",
    "    print(\"-\" * 70)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:<15} | {token.pos_:<20} | {token.tag_:<20} | {spacy.explain(token.tag_)}\")\n",
    "else:\n",
    "    print(\"\\nЧастеречная разметка невозможна без обработанного документа.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340927bd",
   "metadata": {},
   "source": [
    "Для каждого токена была определена его часть речи (например, NOUN - существительное, VERB - глагол, ADP - предлог, PUNCT - знак препинания). token.pos_ дает общую (грубую) метку части речи, а token.tag_ - более детальную (тонкую) метку, специфичную для языка и набора тегов модели. spacy.explain() дает краткое описание метки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88b92d",
   "metadata": {},
   "source": [
    "## 3. Лемматизация\n",
    "Приведение слова к его начальной (словарной) форме — лемме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ee4c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Лемматизация ---\n",
      "Токен           | Лемма           | Часть речи     \n",
      "--------------------------------------------------\n",
      "Мама            | мама            | NOUN\n",
      "мыла            | мыла            | VERB\n",
      "раму            | рама            | NOUN\n",
      "вчера           | вчера           | ADV\n",
      "вечером         | вечер           | NOUN\n",
      "в               | в               | ADP\n",
      "Москве          | москва          | PROPN\n",
      ",               | ,               | PUNCT\n",
      "а               | а               | CCONJ\n",
      "Билл            | билл            | PROPN\n",
      "Гейтс           | гейтс           | PROPN\n",
      "основал         | основать        | VERB\n",
      "компанию        | компания        | NOUN\n",
      "Microsoft       | microsoft       | PROPN\n",
      ".               | .               | PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Вывод токенов с их леммами\n",
    "if doc:\n",
    "    print(\"\\n--- 3. Лемматизация ---\")\n",
    "    print(f\"{'Токен':<15} | {'Лемма':<15} | {'Часть речи':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:<15} | {token.lemma_:<15} | {token.pos_}\")\n",
    "else:\n",
    "    print(\"\\nЛемматизация невозможна без обработанного документа.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eaf170",
   "metadata": {},
   "source": [
    "Каждое слово было приведено к его начальной форме. Например, \"мыла\" стало \"мыть\", \"раму\" - \"рама\", \"основал\" - \"основать\". Это помогает при анализе текста, так как разные формы одного слова объединяются."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7721af1a",
   "metadata": {},
   "source": [
    "## 4. Выделение (распознавание) именованных сущностей (Named Entity Recognition - NER)\n",
    "Нахождение и классификация именованных сущностей в тексте, таких как имена людей, названия организаций, географические объекты, даты и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3caae9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Распознавание именованных сущностей (NER) ---\n",
      "Сущность                       | Тип        | Описание типа\n",
      "----------------------------------------------------------------------\n",
      "Москве                         | LOC        | Non-GPE locations, mountain ranges, bodies of water\n",
      "Билл Гейтс                     | PER        | Named person or family.\n",
      "Microsoft                      | ORG        | Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "# Вывод распознанных именованных сущностей\n",
    "if doc:\n",
    "    print(\"\\n--- 4. Распознавание именованных сущностей (NER) ---\")\n",
    "    if doc.ents:\n",
    "        print(f\"{'Сущность':<30} | {'Тип':<10} | {'Описание типа'}\")\n",
    "        print(\"-\" * 70)\n",
    "        for ent in doc.ents:\n",
    "            print(f\"{ent.text:<30} | {ent.label_:<10} | {spacy.explain(ent.label_)}\")\n",
    "    else:\n",
    "        print(\"Именованные сущности не найдены в тексте.\")\n",
    "else:\n",
    "    print(\"\\nРаспознавание именованных сущностей невозможно без обработанного документа.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbf7a7",
   "metadata": {},
   "source": [
    "Модель нашла и классифицировала именованные сущности. Например, \"Москве\" было распознано как LOC (местоположение), \"Билл Гейтс\" как PER (персона), а \"Microsoft\" как ORG (организация). Точность и типы сущностей зависят от используемой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18115950",
   "metadata": {},
   "source": [
    "## 5. Разбор предложения (Dependency Parsing)\n",
    "Анализ грамматической структуры предложения, определение синтаксических связей между словами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a801213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Разбор предложения (Синтаксические зависимости) ---\n",
      "Токен           | Синт. связь     | Головное слово  | Пояснение связи\n",
      "---------------------------------------------------------------------------\n",
      "Мама            | nsubj           | мыла            | nominal subject\n",
      "мыла            | ROOT            | мыла            | root\n",
      "раму            | obj             | мыла            | object\n",
      "вчера           | advmod          | мыла            | adverbial modifier\n",
      "вечером         | obl             | вчера           | oblique nominal\n",
      "в               | case            | Москве          | case marking\n",
      "Москве          | obl             | мыла            | oblique nominal\n",
      ",               | punct           | основал         | punctuation\n",
      "а               | cc              | основал         | coordinating conjunction\n",
      "Билл            | nsubj           | основал         | nominal subject\n",
      "Гейтс           | flat:name       | Билл            | None\n",
      "основал         | conj            | мыла            | conjunct\n",
      "компанию        | obj             | основал         | object\n",
      "Microsoft       | appos           | компанию        | appositional modifier\n",
      ".               | punct           | мыла            | punctuation\n"
     ]
    }
   ],
   "source": [
    "# Вывод синтаксических связей\n",
    "if doc:\n",
    "    print(\"\\n--- 5. Разбор предложения (Синтаксические зависимости) ---\")\n",
    "    print(f\"{'Токен':<15} | {'Синт. связь':<15} | {'Головное слово':<15} | {'Пояснение связи'}\")\n",
    "    print(\"-\" * 75)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text:<15} | {token.dep_:<15} | {token.head.text:<15} | {spacy.explain(token.dep_)}\")\n",
    "else:\n",
    "    print(\"\\nРазбор предложения невозможен без обработанного документа.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2e368",
   "metadata": {},
   "source": [
    "Для каждого слова (токена) определена его синтаксическая роль в предложении и слово, от которого оно зависит (\"головное слово\"). Например, \"Мама\" может быть подлежащим (nsubj) глагола \"мыла\", \"раму\" - прямым дополнением (obj) глагола \"мыла\". Это помогает понять структуру предложения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0311f",
   "metadata": {},
   "source": [
    "**Визуализация дерева зависимостей**\n",
    "Для этого может понадобиться установка дополнительной библиотеки displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b980579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Визуализация дерева зависимостей (сохранение в HTML) ---\n",
      "Дерево зависимостей сохранено в: c:\\studies\\MMO\\Lab5\\dependency_tree.html\n",
      "Откройте этот файл в браузере для просмотра.\n",
      "\n",
      "--- Визуализация NER (сохранение в HTML) ---\n",
      "Визуализация NER сохранена в: c:\\studies\\MMO\\Lab5\\ner_visualization.html\n",
      "Откройте этот файл в браузере для просмотра.\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "import os # Для работы с путями\n",
    "\n",
    "if doc:\n",
    "    print(\"\\n--- Визуализация дерева зависимостей (сохранение в HTML) ---\")\n",
    "    # Попробуем явно указать jupyter=False\n",
    "    try:\n",
    "        html_dep = displacy.render(doc, style=\"dep\", page=True, jupyter=False) \n",
    "        output_path_dep = \"dependency_tree.html\"\n",
    "        with open(output_path_dep, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html_dep)\n",
    "        print(f\"Дерево зависимостей сохранено в: {os.path.abspath(output_path_dep)}\")\n",
    "        print(\"Откройте этот файл в браузере для просмотра.\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Ошибка при генерации HTML для дерева зависимостей: {e}\")\n",
    "        print(\"Похоже, spaCy все еще пытается использовать компоненты IPython.\")\n",
    "        print(\"Убедитесь, что IPython установлен и обновлен в вашем окружении: pip install --upgrade ipython\")\n",
    "\n",
    "    if doc.ents:\n",
    "        print(\"\\n--- Визуализация NER (сохранение в HTML) ---\")\n",
    "        try:\n",
    "            html_ent = displacy.render(doc, style=\"ent\", page=True, jupyter=False)\n",
    "            output_path_ent = \"ner_visualization.html\"\n",
    "            with open(output_path_ent, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html_ent)\n",
    "            print(f\"Визуализация NER сохранена в: {os.path.abspath(output_path_ent)}\")\n",
    "            print(\"Откройте этот файл в браузере для просмотра.\")\n",
    "        except ImportError as e:\n",
    "            print(f\"Ошибка при генерации HTML для NER: {e}\")\n",
    "            print(\"Похоже, spaCy все еще пытается использовать компоненты IPython.\")\n",
    "            print(\"Убедитесь, что IPython установлен и обновлен в вашем окружении: pip install --upgrade ipython\")\n",
    "    else:\n",
    "        print(\"\\nИменованные сущности не найдены, NER визуализация не создана.\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nВизуализация невозможна: документ не обработан.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b58e2",
   "metadata": {},
   "source": [
    "Этот блок кода (если выполнен в Jupyter) отобразит графическое представление синтаксических связей в предложении, что делает их понимание более наглядным. Стрелки указывают от зависимого слова к головному, а метки на стрелках обозначают тип синтаксической связи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71466f",
   "metadata": {},
   "source": [
    "### Итоговые выводы по лабораторной работе №5\n",
    "\n",
    "В ходе данной лабораторной работы были выполнены основные задачи обработки естественного языка для заданного предложения с использованием библиотеки `spaCy`:\n",
    "\n",
    "1.  **Токенизация:** Предложение было успешно разделено на отдельные лексические единицы (токены), включая слова и знаки препинания.\n",
    "2.  **Частеречная разметка:** Каждому токену была присвоена соответствующая часть речи, что позволяет глубже понять грамматическую структуру текста. Были представлены как общие (Coarse POS), так и более детальные (Fine POS) метки.\n",
    "3.  **Лемматизация:** Все слова были приведены к их начальной словарной форме (лемме), что важно для унификации слов при дальнейшем анализе.\n",
    "4.  **Распознавание именованных сущностей (NER):** В тексте были идентифицированы и классифицированы именованные сущности, такие как имена людей, названия организаций и географические локации.\n",
    "5.  **Разбор предложения (Синтаксические зависимости):** Были проанализированы синтаксические связи между словами в предложении, что позволило выявить грамматическую структуру и отношения между компонентами предложения. Визуализация дерева зависимостей наглядно продемонстрировала эти связи.\n",
    "\n",
    "Использование библиотеки `spaCy` и предварительно обученной языковой модели позволило эффективно и с высокой точностью решить поставленные задачи. Результаты показывают возможности современных NLP-инструментов для анализа текста."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
