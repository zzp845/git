{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9d5ad0",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4: Алгоритм Policy Iteration\n",
    "**Выполнил:** Чжан Цзэнпэн, ИУ5И-21М"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7163e4",
   "metadata": {},
   "source": [
    "## Цель лабораторной работы:\n",
    "Ознакомление с базовыми методами обучения с подкреплением.\n",
    "\n",
    "## Задание:\n",
    "1. На основе рассмотренного на лекции примера реализуйте алгоритм Policy Iteration для любой среды обучения с подкреплением (кроме рассмотренной на лекции среды Toy Text / Frozen Lake) из библиотеки Gym (или аналогичной библиотеки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6b1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym # Обновленная библиотека Gym\n",
    "import numpy as np\n",
    "from pprint import pprint # Для красивого вывода словарей и матриц"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5310d66",
   "metadata": {},
   "source": [
    "### 2. Описание среды: CliffWalking-v0\n",
    "\n",
    "Среда \"CliffWalking-v0\" представляет собой сеточное поле размером 4x12.\n",
    "- **Состояния**: 48 состояний (от 0 до 47), представляющих каждую ячейку сетки.\n",
    "- **Действия**: 4 дискретных действия:\n",
    "    - 0: Двигаться вверх\n",
    "    - 1: Двигаться вправо\n",
    "    - 2: Двигаться вниз\n",
    "    - 3: Двигаться влево\n",
    "- **Начальное состояние**: Агент всегда начинает в ячейке [3, 0] (левый нижний угол, состояние 36).\n",
    "- **Целевое состояние**: Ячейка [3, 11] (правый нижний угол, состояние 47).\n",
    "- **\"Обрыв\"**: Ячейки [3, 1]...[3, 10] (состояния 37-46). Попадание на обрыв возвращает агента в начальное состояние (36).\n",
    "- **Награды**:\n",
    "    - -1 за каждый шаг, не ведущий к обрыву.\n",
    "    - -100 за шаг, приводящий к попаданию на обрыв.\n",
    "- **Завершение эпизода**: Эпизод завершается, когда агент достигает целевого состояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5be0a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пространство состояний (количество): 48\n",
      "Пространство действий (количество): 4\n",
      "\n",
      "Пример матрицы переходов для состояния 36 (старт), действие 0 (вверх):\n",
      "[(1.0, np.int64(24), -1, False)]\n",
      "\n",
      "Пример матрицы переходов для состояния 25, действие 2 (вниз):\n",
      "[(1.0, np.int64(36), -100, False)]\n",
      "\n",
      "Пример матрицы переходов для состояния 46, действие 1 (вправо):\n",
      "[(1.0, np.int64(47), -1, True)]\n",
      "\n",
      "Размерность сетки (env.shape): (4, 12)\n"
     ]
    }
   ],
   "source": [
    "# Инициализация среды\n",
    "env_raw = gym.make(\"CliffWalking-v0\")\n",
    "env = env_raw.unwrapped # <<<--- ДОБАВЛЕНО: получаем доступ к базовой среде\n",
    "\n",
    "# Пространство состояний (количество состояний)\n",
    "n_states = env.observation_space.n # Можно использовать env_raw или env, это свойство обычно доступно\n",
    "print(f\"Пространство состояний (количество): {n_states}\")\n",
    "\n",
    "# Пространство действий (количество действий)\n",
    "n_actions = env.action_space.n # Аналогично\n",
    "print(f\"Пространство действий (количество): {n_actions}\")\n",
    "\n",
    "# Матрица переходов P\n",
    "# Теперь обращаемся к env.P, где env - это \"развернутая\" среда\n",
    "print(\"\\nПример матрицы переходов для состояния 36 (старт), действие 0 (вверх):\")\n",
    "pprint(env.P[36][0])\n",
    "\n",
    "print(\"\\nПример матрицы переходов для состояния 25, действие 2 (вниз):\")\n",
    "pprint(env.P[25][2])\n",
    "\n",
    "print(\"\\nПример матрицы переходов для состояния 46, действие 1 (вправо):\")\n",
    "pprint(env.P[46][1])\n",
    "\n",
    "# Также, для метода print_policy в классе агента нам понадобится env.shape\n",
    "# Убедимся, что он доступен (обычно для сеточных сред он есть в unwrapped версии)\n",
    "try:\n",
    "    print(f\"\\nРазмерность сетки (env.shape): {env.shape}\")\n",
    "except AttributeError:\n",
    "    print(\"\\nАтрибут env.shape не найден. Для CliffWalking это обычно (4,12). Установим вручную, если нужно.\")\n",
    "    # Если env.shape не будет доступен, нам придется его задать вручную в классе агента\n",
    "    # для reshape. Для CliffWalking это (4, 12).\n",
    "    # В классе PolicyIterationAgent можно будет передавать shape или использовать значения по умолчанию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561d501",
   "metadata": {},
   "source": [
    "### 3. Реализация алгоритма Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a5c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationAgent:\n",
    "    def __init__(self, env, gamma=0.99, theta=1e-8):\n",
    "        self.env = env\n",
    "        self.n_states = env.observation_space.n\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.gamma = gamma  # Коэффициент дисконтирования\n",
    "        self.theta = theta  # Малое число для проверки сходимости\n",
    "\n",
    "        # Инициализация случайной политики (равновероятный выбор действий)\n",
    "        self.policy = np.ones([self.n_states, self.n_actions]) / self.n_actions\n",
    "        # Инициализация функции ценности состояний нулями\n",
    "        self.V = np.zeros(self.n_states)\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        \"\"\"\n",
    "        Оценивает текущую политику.\n",
    "        Итеративно вычисляет функцию ценности V для текущей политики.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(self.n_states):\n",
    "                v_s_old = self.V[s]\n",
    "                new_v_s = 0\n",
    "                # Для каждого действия, возможного по текущей политике\n",
    "                for a in range(self.n_actions):\n",
    "                    action_prob = self.policy[s, a]\n",
    "                    # Для каждого возможного исхода действия (в CliffWalking один исход для каждого s,a)\n",
    "                    for prob, next_state, reward, done in self.env.P[s][a]:\n",
    "                        new_v_s += action_prob * prob * (reward + self.gamma * self.V[next_state])\n",
    "                self.V[s] = new_v_s\n",
    "                delta = max(delta, np.abs(v_s_old - self.V[s]))\n",
    "            if delta < self.theta:\n",
    "                break # Сходимость достигнута\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        \"\"\"\n",
    "        Улучшает политику на основе текущей функции ценности V.\n",
    "        Возвращает True, если политика изменилась (стала стабильной), иначе False.\n",
    "        \"\"\"\n",
    "        policy_stable = True\n",
    "        for s in range(self.n_states):\n",
    "            old_action_probs = self.policy[s].copy() # Сохраняем старую политику для состояния s\n",
    "            \n",
    "            # Вычисляем Q-значения для всех действий в состоянии s\n",
    "            q_s_a = np.zeros(self.n_actions)\n",
    "            for a in range(self.n_actions):\n",
    "                for prob, next_state, reward, done in self.env.P[s][a]:\n",
    "                    q_s_a[a] += prob * (reward + self.gamma * self.V[next_state])\n",
    "            \n",
    "            # Находим лучшее(ие) действие(я) - жадный выбор\n",
    "            best_actions = np.argwhere(q_s_a == np.max(q_s_a)).flatten()\n",
    "            \n",
    "            # Обновляем политику для состояния s\n",
    "            # Делаем ее детерминированной, выбирая одно из лучших действий (или распределяя вероятность между ними)\n",
    "            self.policy[s] = np.zeros(self.n_actions)\n",
    "            self.policy[s, best_actions] = 1.0 / len(best_actions) # Равные вероятности для всех лучших действий\n",
    "\n",
    "            # Проверяем, изменилась ли политика\n",
    "            if not np.array_equal(old_action_probs, self.policy[s]):\n",
    "                policy_stable = False\n",
    "        return policy_stable\n",
    "\n",
    "    def policy_iteration(self, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Основной цикл алгоритма Policy Iteration.\n",
    "        \"\"\"\n",
    "        for i in range(max_iterations):\n",
    "            print(f\"Итерация {i+1}\")\n",
    "            self.policy_evaluation()\n",
    "            policy_stable = self.policy_improvement()\n",
    "            if policy_stable:\n",
    "                print(f\"Политика стабилизировалась на итерации {i+1}.\")\n",
    "                break\n",
    "        if i == max_iterations - 1:\n",
    "             print(f\"Достигнуто максимальное количество итераций ({max_iterations}).\")\n",
    "\n",
    "    def print_policy(self):\n",
    "        print(\"Текущая политика (вероятности действий для каждого состояния):\")\n",
    "        # Для наглядности можно вывести только выбранное действие, если политика детерминирована\n",
    "        optimal_actions = np.argmax(self.policy, axis=1)\n",
    "        policy_grid = optimal_actions.reshape(self.env.shape) # env.shape для CliffWalking (4,12)\n",
    "        \n",
    "        action_symbols = {0: '^ (вверх)', 1: '> (вправо)', 2: 'v (вниз)', 3: '< (влево)'}\n",
    "        \n",
    "        print(\"Оптимальные действия на сетке:\")\n",
    "        for r in range(policy_grid.shape[0]):\n",
    "            row_str = []\n",
    "            for c in range(policy_grid.shape[1]):\n",
    "                state = r * policy_grid.shape[1] + c\n",
    "                if state in [37,38,39,40,41,42,43,44,45,46]: # Обрыв\n",
    "                     row_str.append(\" C \") # Cliff\n",
    "                elif state == 47: # Цель\n",
    "                     row_str.append(\" G \") # Goal\n",
    "                else:\n",
    "                    row_str.append(f\"{action_symbols[policy_grid[r,c]][0]:^3}\") # Используем только символ\n",
    "            print(\" \".join(row_str))\n",
    "        # pprint(self.policy) # Можно раскомментировать для вывода полной матрицы\n",
    "\n",
    "    def print_value_function(self):\n",
    "        print(\"\\nФункция ценности состояний (V):\")\n",
    "        pprint(self.V.reshape(self.env.shape)) # env.shape для CliffWalking (4,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe50d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начальная (случайная) политика:\n",
      "Текущая политика (вероятности действий для каждого состояния):\n",
      "Оптимальные действия на сетке:\n",
      " ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^ \n",
      " ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^ \n",
      " ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^ \n",
      " ^   C   C   C   C   C   C   C   C   C   C   G \n",
      "Итерация 1\n",
      "Итерация 2\n",
      "Итерация 3\n",
      "Итерация 4\n",
      "Итерация 5\n",
      "Итерация 6\n",
      "Итерация 7\n",
      "Итерация 8\n",
      "Итерация 9\n",
      "Итерация 10\n",
      "Итерация 11\n",
      "Итерация 12\n",
      "Итерация 13\n",
      "Итерация 14\n",
      "Итерация 15\n",
      "Итерация 16\n",
      "Итерация 17\n",
      "Итерация 18\n",
      "Итерация 19\n",
      "Итерация 20\n",
      "Итерация 21\n",
      "Итерация 22\n",
      "Итерация 23\n",
      "Итерация 24\n",
      "Итерация 25\n",
      "Итерация 26\n",
      "Итерация 27\n",
      "Итерация 28\n",
      "Итерация 29\n",
      "Итерация 30\n",
      "Итерация 31\n",
      "Итерация 32\n",
      "Итерация 33\n",
      "Итерация 34\n",
      "Итерация 35\n",
      "Итерация 36\n",
      "Итерация 37\n",
      "Итерация 38\n",
      "Итерация 39\n",
      "Итерация 40\n",
      "Итерация 41\n",
      "Итерация 42\n",
      "Итерация 43\n",
      "Итерация 44\n",
      "Итерация 45\n",
      "Итерация 46\n",
      "Итерация 47\n",
      "Итерация 48\n",
      "Итерация 49\n",
      "Итерация 50\n",
      "Итерация 51\n",
      "Итерация 52\n",
      "Итерация 53\n",
      "Итерация 54\n",
      "Итерация 55\n",
      "Итерация 56\n",
      "Итерация 57\n",
      "Итерация 58\n",
      "Итерация 59\n",
      "Итерация 60\n",
      "Итерация 61\n",
      "Итерация 62\n",
      "Итерация 63\n",
      "Итерация 64\n",
      "Итерация 65\n",
      "Итерация 66\n",
      "Итерация 67\n",
      "Итерация 68\n",
      "Итерация 69\n",
      "Итерация 70\n",
      "Итерация 71\n",
      "Итерация 72\n",
      "Итерация 73\n",
      "Итерация 74\n",
      "Итерация 75\n",
      "Итерация 76\n",
      "Итерация 77\n",
      "Итерация 78\n",
      "Итерация 79\n",
      "Итерация 80\n",
      "Итерация 81\n",
      "Итерация 82\n",
      "Итерация 83\n",
      "Итерация 84\n",
      "Итерация 85\n",
      "Итерация 86\n",
      "Итерация 87\n",
      "Итерация 88\n",
      "Итерация 89\n",
      "Итерация 90\n",
      "Итерация 91\n",
      "Итерация 92\n",
      "Итерация 93\n",
      "Итерация 94\n",
      "Политика стабилизировалась на итерации 94.\n",
      "\n",
      "Оптимальная политика:\n",
      "Текущая политика (вероятности действий для каждого состояния):\n",
      "Оптимальные действия на сетке:\n",
      " ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^ \n",
      " ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^ \n",
      " ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^   ^ \n",
      " ^   C   C   C   C   C   C   C   C   C   C   G \n",
      "\n",
      "Функция ценности состояний (V):\n",
      "array([[-10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10.,\n",
      "        -10.],\n",
      "       [-10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10.,\n",
      "        -10.],\n",
      "       [-10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10.,\n",
      "        -10.],\n",
      "       [-10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10.,\n",
      "        -10.]])\n"
     ]
    }
   ],
   "source": [
    "# Создание и обучение агента\n",
    "# Передаем РАЗВЕРНУТУЮ среду в агент!\n",
    "agent = PolicyIterationAgent(env_raw.unwrapped, gamma=0.9) # env_raw.unwrapped здесь\n",
    "print(\"Начальная (случайная) политика:\")\n",
    "agent.print_policy() # Этот метод внутри агента должен использовать self.env.shape\n",
    "\n",
    "agent.policy_iteration(max_iterations=100)\n",
    "\n",
    "print(\"\\nОптимальная политика:\")\n",
    "agent.print_policy()\n",
    "agent.print_value_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b721937",
   "metadata": {},
   "source": [
    "### 4. Проигрывание сцены с обученным агентом (опционально, но интересно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8840e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для проигрывания сцены\n",
    "def play_episode(env_play, agent_play, render=True, max_steps=100):\n",
    "    \"\"\"Проигрывает один эпизод с использованием политики агента.\"\"\"\n",
    "    state, _ = env_play.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(\"\\n--- Начало нового эпизода ---\")\n",
    "    if render:\n",
    "        env_play.render() # Для текстового вывода в консоль\n",
    "        # Для графического вывода (если среда поддерживает и настроено):\n",
    "        # img = plt.imshow(env_play.render(mode='rgb_array'))\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Выбираем действие согласно политике агента\n",
    "        # np.random.choice используется, если политика стохастическая (несколько лучших действий)\n",
    "        action = np.random.choice(np.arange(agent_play.n_actions), p=agent_play.policy[state])\n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env_play.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if render:\n",
    "            print(f\"Шаг: {step+1}, Состояние: {state}, Действие: {action}, Награда: {reward}, Новое состояние: {next_state}\")\n",
    "            # Для текстового вывода\n",
    "            env_play.render() \n",
    "            # if 'human' in env_play.metadata.get('render_modes', []):\n",
    "            #     env_play.render()\n",
    "            # elif 'rgb_array' in env_play.metadata.get('render_modes', []):\n",
    "            #      img.set_data(env_play.render(mode='rgb_array'))\n",
    "            #      plt.pause(0.1) # небольшая задержка для отображения\n",
    "            #      plt.draw()\n",
    "\n",
    "\n",
    "        state = next_state\n",
    "        if terminated or truncated:\n",
    "            print(f\"Эпизод завершен после {step+1} шагов.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Итоговая награда за эпизод: {total_reward}\")\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a3c9e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Начало нового эпизода ---\n",
      "Шаг: 1, Состояние: 36, Действие: 0, Награда: -1, Новое состояние: 24\n",
      "Шаг: 2, Состояние: 24, Действие: 2, Награда: -1, Новое состояние: 36\n",
      "Шаг: 3, Состояние: 36, Действие: 3, Награда: -1, Новое состояние: 36\n",
      "Шаг: 4, Состояние: 36, Действие: 2, Награда: -1, Новое состояние: 36\n",
      "Шаг: 5, Состояние: 36, Действие: 0, Награда: -1, Новое состояние: 24\n",
      "Шаг: 6, Состояние: 24, Действие: 3, Награда: -1, Новое состояние: 24\n",
      "Шаг: 7, Состояние: 24, Действие: 1, Награда: -1, Новое состояние: 25\n",
      "Шаг: 8, Состояние: 25, Действие: 1, Награда: -1, Новое состояние: 26\n",
      "Шаг: 9, Состояние: 26, Действие: 3, Награда: -1, Новое состояние: 25\n",
      "Шаг: 10, Состояние: 25, Действие: 0, Награда: -1, Новое состояние: 13\n",
      "Шаг: 11, Состояние: 13, Действие: 1, Награда: -1, Новое состояние: 14\n",
      "Шаг: 12, Состояние: 14, Действие: 0, Награда: -1, Новое состояние: 2\n",
      "Шаг: 13, Состояние: 2, Действие: 3, Награда: -1, Новое состояние: 1\n",
      "Шаг: 14, Состояние: 1, Действие: 2, Награда: -1, Новое состояние: 13\n",
      "Шаг: 15, Состояние: 13, Действие: 1, Награда: -1, Новое состояние: 14\n",
      "Шаг: 16, Состояние: 14, Действие: 0, Награда: -1, Новое состояние: 2\n",
      "Шаг: 17, Состояние: 2, Действие: 2, Награда: -1, Новое состояние: 14\n",
      "Шаг: 18, Состояние: 14, Действие: 1, Награда: -1, Новое состояние: 15\n",
      "Шаг: 19, Состояние: 15, Действие: 1, Награда: -1, Новое состояние: 16\n",
      "Шаг: 20, Состояние: 16, Действие: 2, Награда: -1, Новое состояние: 28\n",
      "Шаг: 21, Состояние: 28, Действие: 3, Награда: -1, Новое состояние: 27\n",
      "Шаг: 22, Состояние: 27, Действие: 1, Награда: -1, Новое состояние: 28\n",
      "Шаг: 23, Состояние: 28, Действие: 0, Награда: -1, Новое состояние: 16\n",
      "Шаг: 24, Состояние: 16, Действие: 3, Награда: -1, Новое состояние: 15\n",
      "Шаг: 25, Состояние: 15, Действие: 0, Награда: -1, Новое состояние: 3\n",
      "Шаг: 26, Состояние: 3, Действие: 0, Награда: -1, Новое состояние: 3\n",
      "Шаг: 27, Состояние: 3, Действие: 1, Награда: -1, Новое состояние: 4\n",
      "Шаг: 28, Состояние: 4, Действие: 2, Награда: -1, Новое состояние: 16\n",
      "Шаг: 29, Состояние: 16, Действие: 0, Награда: -1, Новое состояние: 4\n",
      "Шаг: 30, Состояние: 4, Действие: 1, Награда: -1, Новое состояние: 5\n",
      "Шаг: 31, Состояние: 5, Действие: 1, Награда: -1, Новое состояние: 6\n",
      "Шаг: 32, Состояние: 6, Действие: 2, Награда: -1, Новое состояние: 18\n",
      "Шаг: 33, Состояние: 18, Действие: 1, Награда: -1, Новое состояние: 19\n",
      "Шаг: 34, Состояние: 19, Действие: 3, Награда: -1, Новое состояние: 18\n",
      "Шаг: 35, Состояние: 18, Действие: 3, Награда: -1, Новое состояние: 17\n",
      "Шаг: 36, Состояние: 17, Действие: 0, Награда: -1, Новое состояние: 5\n",
      "Шаг: 37, Состояние: 5, Действие: 2, Награда: -1, Новое состояние: 17\n",
      "Шаг: 38, Состояние: 17, Действие: 2, Награда: -1, Новое состояние: 29\n",
      "Шаг: 39, Состояние: 29, Действие: 3, Награда: -1, Новое состояние: 28\n",
      "Шаг: 40, Состояние: 28, Действие: 1, Награда: -1, Новое состояние: 29\n",
      "Шаг: 41, Состояние: 29, Действие: 0, Награда: -1, Новое состояние: 17\n",
      "Шаг: 42, Состояние: 17, Действие: 2, Награда: -1, Новое состояние: 29\n",
      "Шаг: 43, Состояние: 29, Действие: 1, Награда: -1, Новое состояние: 30\n",
      "Шаг: 44, Состояние: 30, Действие: 1, Награда: -1, Новое состояние: 31\n",
      "Шаг: 45, Состояние: 31, Действие: 0, Награда: -1, Новое состояние: 19\n",
      "Шаг: 46, Состояние: 19, Действие: 0, Награда: -1, Новое состояние: 7\n",
      "Шаг: 47, Состояние: 7, Действие: 2, Награда: -1, Новое состояние: 19\n",
      "Шаг: 48, Состояние: 19, Действие: 0, Награда: -1, Новое состояние: 7\n",
      "Шаг: 49, Состояние: 7, Действие: 2, Награда: -1, Новое состояние: 19\n",
      "Шаг: 50, Состояние: 19, Действие: 2, Награда: -1, Новое состояние: 31\n",
      "Итоговая награда за эпизод: -50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\studies\\MMO\\shared_venv\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\cliffwalking.py:218: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CliffWalking-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Запуск проигрывания\n",
    "# Создаем среду для проигрывания (можно с другим режимом рендера, если нужно)\n",
    "# CliffWalking-v0 не имеет встроенного графического рендера, только текстовый (mode='ansi') или rgb_array\n",
    "# Для текстового вывода в Jupyter, print(env.render()) может быть лучше чем env.render() напрямую.\n",
    "# Либо использовать параметр render_mode='human' при создании среды для вывода в отдельном окне (если поддерживается).\n",
    "# Но в Google Colab или простом скрипте 'human' может не работать.\n",
    "\n",
    "# Для текстового вывода в консоли/Jupyter, env.render() внутри play_episode уже есть.\n",
    "# Если используете Jupyter и хотите видеть \"картинку\" (rgb_array), то нужен matplotlib\n",
    "# и код как закомментировано выше. Но для CliffWalking rgb_array не самый информативный.\n",
    "\n",
    "# Давайте просто запустим с текстовым выводом\n",
    "env_to_play = gym.make(\"CliffWalking-v0\", render_mode=None) # render_mode='ansi' для цветного текста, если терминал поддерживает\n",
    "\n",
    "# Проиграем несколько эпизодов\n",
    "for _ in range(1): # Можно увеличить количество эпизодов\n",
    "    play_episode(env_to_play, agent, render=True, max_steps=50) # render=True включает print(env.render())\n",
    "\n",
    "env.close()\n",
    "env_to_play.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMO",
   "language": "python",
   "name": "mmo_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
